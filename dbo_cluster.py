# dbo_cluster.py
import torch
from dbo_net.model import DBONet
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np
import torch.nn.functional as F
from dbo_loss import reconstruction_loss, sparsity_loss, view_alignment_loss
from sklearn.decomposition import PCA


class DBOClusterer:
    def __init__(self, n_clusters=2, device='cpu', n_epochs=10, lr=1e-3):
        self.n_clusters = n_clusters
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.n_epochs = n_epochs
        self.lr = lr

    def build_model(self, view_dims, n_blocks=2):
        self.model = DBONet(
            nfeats=view_dims,
            n_view=len(view_dims),
            n_clusters=self.n_clusters,
            blocks=n_blocks,
            device=self.device
        ).to(self.device)

    def train(self, features):
        self.model.train()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)

        for epoch in range(self.n_epochs):
            optimizer.zero_grad()

            Z_list = self.model.encode_each_view(features)  # è·å–æ¯ä¸ªè§†å›¾çš„ Z
            Z_concat = self.model(features)  # å¤šè§†å›¾èåˆåçš„ Zï¼ˆç”¨äºèšç±»ï¼‰

            loss = 0
            # é€è§†å›¾æŸå¤±
            for i in range(len(features)):
                Xi = features[i]
                Zi = Z_list[i]
                Ui = self.model.U[i]

                loss += reconstruction_loss(Xi, Ui, Zi)  # æ˜¾å¼é‡æ„é¡¹
                loss += sparsity_loss(Zi, l1_weight=1e-3)  # ç¨€ç–çº¦æŸ

            # å¤šè§†å›¾å¯¹é½æŸå¤±
            loss += view_alignment_loss(Z_list)

            loss.backward()
            optimizer.step()

    def cluster(self, peer_views):
        n_peers = len(peer_views)
        n_views = len(peer_views[0]) if n_peers > 0 else 0
        features = []
        MAX_FEATURE_DIM = 512

        # print("[Debug] â–¶ï¸ å¼€å§‹æ„é€ ç‰¹å¾è§†å›¾ï¼Œn_peers =", n_peers, ", n_views =", n_views)

        for v in range(n_views):
            view_data_raw = [peer_views[i][v] for i in range(n_peers)]

            # æ£€æŸ¥æ¯ä¸ª peer çš„è¯¥è§†å›¾æ˜¯å¦ä¸ºåˆæ³• array
            for i, vdata in enumerate(view_data_raw):
                if vdata is None:
                    raise ValueError(f"[Error] Peer {i} çš„è§†å›¾ {v} ä¸º Noneï¼Œè¯·æ£€æŸ¥ participant_update è¿”å›å€¼")
                if not hasattr(vdata, 'shape'):
                    raise ValueError(f"[Error] Peer {i} çš„è§†å›¾ {v} é arrayï¼Œç±»å‹ä¸º {type(vdata)}")

            try:
                view_data = np.stack([x.flatten() for x in view_data_raw], axis=0)  # [n_peers, feature_dim]
            except Exception as e:
                raise RuntimeError(f"[Error] stacking ç¬¬ {v} ä¸ªè§†å›¾å¤±è´¥: {e}")

            # print(f"[Debug] âœ”ï¸ è§†å›¾ {v} shape: {view_data.shape}")
            if view_data.shape[1] > MAX_FEATURE_DIM:
                from sklearn.decomposition import PCA
                max_pca_dim = min(MAX_FEATURE_DIM, view_data.shape[0])
                # print(f"[Debug] âš ï¸ View {v} ç»´åº¦å¤ªå¤§ ({view_data.shape[1]}), æ‰§è¡Œ PCA é™ç»´è‡³ {max_pca_dim}")
                pca = PCA(n_components=max_pca_dim)
                view_data = pca.fit_transform(view_data)

            view_data = StandardScaler().fit_transform(view_data)
            view_tensor = torch.tensor(view_data, dtype=torch.float32, device=self.device)
            features.append(view_tensor)

        # print("[Debug] âœ… æ‰€æœ‰è§†å›¾æ„å»ºå®Œæˆ")
        # for idx, f in enumerate(features):
            # print(f" - features[{idx}].shape = {f.shape}")

        if self.model is None:
            view_dims = [f.shape[1] for f in features]
            # print("[Debug] ğŸš€ æ„å»ºæ¨¡å‹ï¼Œview_dims =", view_dims)
            assert all(d > 0 for d in view_dims), "ğŸš¨ æœ‰è§†å›¾ç»´åº¦ä¸º0ï¼ŒU åˆå§‹åŒ–å°†å¤±è´¥"
            self.build_model(view_dims)

        # === Step 1: è®­ç»ƒæ¨¡å‹ï¼Œä¼˜åŒ– latent è¡¨å¾ ===
        self.train(features)

        # === Step 2: èšç±»å¾—åˆ°æ ‡ç­¾ ===
        self.model.eval()
        with torch.no_grad():
            Z = self.model(features)
            pred = self.model.get_cluster_labels(Z)

        return pred.cpu().numpy().tolist()

